---
title: "Econometrics II TA Session #3"
author: "Hiroki Kato"
output:
  pdf_document:
    latex_engine: xelatex
    md_extensions: +raw_attribute
    number_sections: true
    extra_dependencies: ["xcolor"]
    keep_tex: yes
fontsize: 12pt
header-includes:
  - \usepackage{zxjatype}
  - \setCJKmainfont[BoldFont = IPAゴシック]{IPA明朝}
  - \setCJKsansfont{IPAゴシック}
  - \setCJKmonofont{IPAゴシック}
  - \parindent = 1em
  - \newcommand{\argmax}{\mathop{\rm arg~max}\limits}
  - \newcommand{\argmin}{\mathop{\rm arg~min}\limits}
  - \DeclareMathOperator*{\plim}{plim}
---

```{r setup, include = FALSE, echo = FALSE, purl = FALSE}
# This is options to make pdf file. You should ignore
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  echo = TRUE, 
  cache = FALSE,
  fig.pos = "h")
knitr::opts_knit$set(root.dir = "C:/Users/katoo/Desktop/2020EconometricsTA/R")
```

# Empirical Application of Binary Model: Titanic Survivors

**Brief Background**.
"Women and children first" is a behavioral norm,
which women and children are saved first in a life-threatening situation.
This code was made famous by the sinking of the Titanic in 1912.
An empirical application investigates characteristics of survivors of Titanic
to answer whether crews obeyed the code or not.

\noindent
**Data**.
We use an open data about Titanic survivors [^source].
Although this dataset contains many variables,
we use only four variables: `survived`, `age`, `fare`, and `sex`.
We summarize descpritons of variables as follows:

- `survived`: a binary variable taking 1 if a passenger survived.
- `age`: a continuous variable representing passeger's age.
- `fare`: a continuous variable representing how much passeger paid.
- `sex`: a string variable representing passenger's sex.

Using `sex`, we will make a binary variable, called `female`, taking 1 if passeger is female.
Intead of `sex`, we use `female` variable in regression.

[^source]: data source: <http://biostat.mc.vanderbilt.edu/DataSets>.

```{r data}
dt <- read.csv(
  file = "./data/titanic.csv", 
  header = TRUE,  sep = ",", row.names = NULL,  stringsAsFactors = FALSE)

dt$female <- ifelse(dt$sex == "female", 1, 0)
dt <- subset(dt, !is.na(survived)&!is.na(age)&!is.na(fare)&!is.na(female))

dt <- dt[,c("survived", "age", "fare", "female")]
head(dt)
```

\noindent
**Model**.
In a binary model, a dependent (outcome) variable $y_i$ takes only two values, i.e., $y_i \in \{0, 1\}$.
A binary variable is sometimes called a *dummy* variable.
In this application, the outcome variable is `survived`.
Explanatory variables are `female`, `age`, and `fare`.
The regression function is 
\begin{equation*}
  \begin{split}
    &\mathbb{E}[survived | female, age, fare] \\
    =& \mathbb{P}[survived = 1 | female, age, fare]
    = G(\beta_0 + \beta_1 female + \beta_2 age + \beta3 fare).
  \end{split}
\end{equation*}
The function $G(\cdot)$ is arbitrary function. In practice, we often use following three specifications:

- Linear probability model (LPM): $G(\mathbf{x}_i \beta) = \mathbf{x}_i \beta$.
- Probit model: $G(\mathbf{x}_i \beta) = \Phi(\mathbf{x}_i \beta)$ where $\Phi(\cdot)$ is the standard Gaussian cumulative function.
- Logit model: $G(\mathbf{x}_i \beta) = 1/(1 + \exp(-\mathbf{x}_i \beta))$.

## Linear Probability Model

The linear probability model specifys that $G(a)$ is linear in $a$, that is, 
\begin{equation*}
  \mathbb{P}[survived = 1 | female, age, fare]
  = \beta_0 + \beta_1 female + \beta_2 age + \beta3 fare.
\end{equation*}
This model can be estimated using the OLS method.
In `R`, we can use the OLS method, running `lm()` function.

```{R ols}
model <- survived ~ female + age + fare
LPM <- lm(model, data = dt)
```
However, `lm()` function does not deal with heteroskedasticity problem.
To resolve it, we need to claculate heteroskedasticity-robust standard errors using the White method.
\begin{equation*}
  \hat{V}(\hat{\beta}) =
  \left( \frac{1}{n} \sum_i \mathbf{x}'_i \mathbf{x}_i  \right)^{-1}
  \left( \frac{1}{n} \sum_i \hat{u}_i^2 \mathbf{x}'_i \mathbf{x}_i \right)
  \left( \frac{1}{n} \sum_i \mathbf{x}'_i \mathbf{x}_i \right)^{-1}
\end{equation*}

```{r RobustSE}
# heteroskedasticity-robust standard errors
dt$"(Intercept)" <- 1
X <- as.matrix(dt[,c("(Intercept)", "female", "age", "fare")])
u <- diag(LPM$residuals^2)

XX <- t(X) %*% X
avgXX <- XX * nrow(X)^{-1}
inv_avgXX <- solve(avgXX)

uXX <- t(X) %*% u %*% X
avguXX <- uXX * nrow(X)^{-1} 

vcov_b <- (inv_avgXX %*% avguXX %*% inv_avgXX) * nrow(X)^{-1}
rse_b <- sqrt(diag(vcov_b))

# homoskedasticity-based standard errors
se_b <- sqrt(diag(vcov(LPM)))

print("The Variance of OLS"); vcov(LPM)
print("The Robust variance of OLS"); vcov_b
print("The Robust se using White method"); rse_b
print("The Robust t-value using White method"); coef(LPM)/rse_b
```

Using the package `lmtest` and `sandwich` is 
the easiest way to calculate heteroskedasticity-robust standard errors and $t$-statistics.

```{r lmtest}
library(lmtest) #use function `coeftest`
library(sandwich) #use function `vcovHC`
coeftest(LPM, vcov = vcovHC(LPM, type = "HC0"))[, "Std. Error"]
coeftest(LPM, vcov = vcovHC(LPM, type = "HC0"))[, "t value"]
```

Finally, we summarize results of linear probability model in table \ref{LPM}.
We will discuss interpretation of results and goodness-of-fit of LPM later.

```{r LPM_result, results = "asis"}
# t-stats
t_b <- coef(LPM)/se_b 
rt_b <- coef(LPM)/rse_b
# p-value Pr( > |t|)
p_b <- pt(abs(t_b), df = nrow(X)-ncol(X), lower = FALSE)*2
rp_b <- pt(abs(rt_b), df = nrow(X)-ncol(X), lower = FALSE)*2

library(stargazer)
stargazer(
  LPM, LPM,
  se = list(se_b, rse_b), t = list(t_b, rt_b), p = list(p_b, rp_b),
  t.auto = FALSE, p.auto = FALSE,
  report = "vcstp", keep.stat = c("n"),
  add.lines = list(
    c("Standard errors", "Homoskedasticity-based", "Heteroskedasticity-robust")),
  title = "Results of Linear Probability Model", label = "LPM",
  type = "latex", header = FALSE, font.size = "small",
  omit.table.layout = "n", table.placement = "h"
)
```

## Probit and Logit Model

Unlike LPM, the probit and logit model must be estimated using the ML method.
The probability of observing $y_i$ is 
\begin{equation*}
  p_{\beta}(y_i|\mathbf{x}_i)  
  = \mathbb{P}(y_i = 1 | x_i)^{y_i} [1 - \mathbb{P}(y_i = 1 | x_i)]^{1-y_i}
  = G(\mathbf{x}_i \beta)^{y_i} (1 - G(\mathbf{x}_i \beta))^{1-y_i}.
\end{equation*}
Taking logalithm yields
\begin{equation*}
  \log p_{\beta}(y_i|\mathbf{x}_i) = y_i \log(G(\mathbf{x}_i \beta)) + (1 - y_i)\log(1 - G(\mathbf{x}_i \beta)).
\end{equation*}
The log-likelihood is 
\begin{equation*}
  M_n(\beta) = \sum_{i=1}^n \log p_{\beta}(y_i|\mathbf{x}_i).
\end{equation*}

The MLE $\hat{\beta}$ holds that the score, which is the first-order derivatives with respect to $\beta$, is equal to 0.
That is $\nabla_{\beta} M_n(\hat{\beta}) = 0$. 
For both logit and probit model, 
the Hessian matrix, $\nabla^2_{\beta\beta'} M_n(\beta)$, is always negative definite.
This implies that log-likelihood function based on both models is grobally concave,
and ensures that the MLE maximizes the log-likelihood function.
The first-order condition of the probit model is 
\begin{equation*}
  \nabla_{\beta} M_n(\hat{\beta}) 
  = \sum_{i = 1}^n \left( y_i - \Phi(\mathbf{x}_i \hat{\beta}) \right) 
  \frac{\phi(\mathbf{x}_i \hat{\beta})}{\Phi(\mathbf{x}_i \hat{\beta})(1 - \phi(\mathbf{x}_i \hat{\beta}))} = 0.
\end{equation*}
The first-order condition of the logit model is 
\begin{equation*}
  \nabla_{\beta} M_n(\hat{\beta}) 
  = \sum_{i = 1}^n \left( y_i - G(\mathbf{x}_i \hat{\beta}) \right) \mathbf{x}'_i = 0.
\end{equation*}
Since it is hard for us to solve this condition analytically,
we obtain estimators using numerical procedure. 

In `R`, the function `nlm()` provides the Newton-Raphson algorithm to minimize the function [^optim].
To run this function, we need to define the log-likelihood function (`LogLik`) beforehand.
Moreover, since we need to give initial values in augments, we use coefficients estimated by OLS.
Alternatively, we often use `glm()` function.
Using this function, we do not need to define the log-likelihood function and initial values.
Since estimates of `glm()` are approximate to estiamtes of `nlm()`,
we can use this function fairly.
In this application, we use `nlm` function to minimize the log-likelihood function.

[^optim]: `optim()` function is an another way to minimize the function. Especially, the function `optim(method = "BFGS")` provides the Quasi-Newton algorithm which carries on the spirit of Newton method.

```{r probit}
Y <- dt$survived
female <- dt$female; age <- dt$age; fare <- dt$fare

# log-likelihood
LnLik <- function(b, model = c("probit", "logit")) {

  xb <- b[1]+ b[2]*female + b[3]*age + b[4]*fare

  if (model == "probit") {
    L <- pnorm(xb)
  } else {
    L <- 1/(1 + exp(-xb))
  }

  LL_i <- Y * log(L) + (1 - Y) * log(1 - L)
  LL <- -sum(LL_i)

  return(LL)
}

#Newton-Raphson
init <- c(0.218, 0.512, -0.002, 0.001)
probit <- nlm(LnLik, init, model = "probit", hessian = TRUE)

label <- c("(Intercept)", "female", "age", "fare")
names(probit$estimate) <- label
colnames(probit$hessian) <- label; rownames(probit$hessian) <- label

b_probit <- probit$estimate
vcov_probit <- solve(probit$hessian); se_probit <- sqrt(diag(vcov_probit))
LL_probit <- -probit$minimum

#glm function
model <- survived ~ female + age + fare
probit_glm <- glm(model, data = dt, family = binomial("probit"))

#result
print("The MLE of probit model using nlm"); b_probit
print("The Variance of probit model using nlm"); vcov_probit
print("The se of probit model using nlm"); se_probit
print("The coefficients of probit using glm"); coef(probit_glm)
print("The se of probit using glm"); sqrt(diag(vcov(probit_glm)))
```

Using `LogLik`, we can also estimate logit model by Newton-Raphson algorithm.
To compare result, we also use `glm()` function.

```{r logit}
#Newton-Raphson
logit <- nlm(LnLik, init, model = "logit", hessian = TRUE)

names(logit$estimate) <- label
colnames(logit$hessian) <- label; rownames(logit$hessian) <- label

b_logit <- logit$estimate
vcov_logit <- solve(logit$hessian); se_logit <- sqrt(diag(vcov_logit))
LL_logit <- -logit$minimum

#glm function
logit_glm <- glm(model, data = dt, family = binomial("logit"))

#result
print("The MLE of logit model"); b_logit
print("The Variance of logit model"); vcov_logit
print("The se of logit model"); se_logit
print("The coefficients of logit using glm"); coef(logit_glm)
print("The se of logit using glm"); sqrt(diag(vcov(logit_glm)))
```

As a result, table \ref{probit_logit} summarizes results of probit model and logit model.
$t$-statistics represents $z$-value which follows the standard normal distribution.
Standard errors are in parentheses.
We will discuss interpretation of results and goodness-of-fit later.

```{r summary_probit_logit, results = "asis"}
# z-value
z_probit <- b_probit/se_probit
z_logit <- b_logit/se_logit

# Pr(>|z|)
p_probit <- pnorm(abs(z_probit), lower = FALSE)*2
p_logit <- pnorm(abs(z_logit), lower = FALSE)*2

stargazer(
  probit_glm, logit_glm,
  coef = list(b_probit, b_logit), se = list(se_probit, se_logit),
  t = list(z_probit, z_logit), p = list(p_probit, p_logit),
  t.auto = FALSE, p.auto = FALSE,
  report = "vcstp", keep.stat = c("n"),
  add.lines = list(
    c("Log-Likelihood", round(LL_probit, 3), round(LL_logit, 3))),
  title = "Results of Probit and Logit model",
  label = "probit_logit",
  type = "latex", header = FALSE, font.size = "small",
  table.placement = "h", omit.table.layout = "n"
)
```