---
title: "Econometrics II TA Session #8"
author: "Hiroki Kato"
output:
  pdf_document:
    latex_engine: xelatex
    md_extensions: +raw_attribute
    number_sections: true
    extra_dependencies: ["xcolor"]
    keep_tex: yes
fontsize: 12pt
header-includes:
  - \usepackage{zxjatype}
  - \setCJKmainfont[BoldFont = IPAゴシック]{IPA明朝}
  - \setCJKsansfont{IPAゴシック}
  - \setCJKmonofont{IPAゴシック}
  - \parindent = 1em
  - \newcommand{\argmax}{\mathop{\rm arg~max}\limits}
  - \newcommand{\argmin}{\mathop{\rm arg~min}\limits}
  - \DeclareMathOperator*{\plim}{plim}
---

```{r setup, include = FALSE, echo = FALSE, purl = FALSE}
# This is options to make pdf file. You should ignore
library(here)
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  echo = TRUE, 
  cache = FALSE,
  fig.pos = "h")
knitr::opts_knit$set(root.dir = paste(here(), "/R", sep = ""))
```


# Empirical Application of Panel Data Model: Earnings Equation

## Backgruond

A researcher wants to estimate the effect of full-time work experience on wages.
He uses a *balanced* panel of 595 individuals from 1976 to 1982, taken from the Panel Study of Income Dynamics (PSID).
The *balanced* panel data means that we can observe all individuals every year.

```{r data}
dt <- read.csv("./data/wages.csv")
head(dt, 14)
```

The variable `id` and `time` indicate individual and time indexs.
We use these two variables to apply panel data models.
Additionally, we use the following variables:

- `exp`: years of full-time work experience
- `sqexp`: squared value of `exp`
- `lwage`: logarithm of wage

```{r summary}
dt <- dt[,c("id", "time", "exp", "lwage")]
dt$sqexp <- dt$exp^2
summary(dt)
```

To examine the effect of labor experience on wages,
we want to estimate the following linear panel data model:

$$
  \text{lwage}_{it} = 
  \beta_1 \cdot \text{exp}_{it} +
  \beta_2 \cdot \text{sqexp}_{it} + 
  u_{it}.
$$

We can define the regression equation as the `formula` object in `R`.
To exclude the intercept, we must specify `-1` in the rhs of regression equation.
Thus, in `R`, we define the linear panel data model as follows:

```{r model}
model <- lwage ~ -1 + exp + sqexp
```

## Pooled OLSE

We want to estimate the above regression equation by the OLS method.
We will discuss assumptions for implementation.
Let $\mathbf{X}_{it}$ be a $1 \times K$ (stochastic) explanatory vector.
This vector contains `exp`, `sqexp`.
Let $Y_{it}$ be a random variable of outcome, that is `lwage`.
Then, the linear panel data model can be rewritten as follows:

$$
  Y_{it} = \mathbf{X}_{it} \beta + u_{it}, \quad t = 1, \ldots, T, \quad i = 1, \ldots, n.
$$

Using notations $\underline{\mathbf{X}}_i = (\mathbf{X}'_{i1}, \ldots, \mathbf{X}'_{iT})'$
and $\underline{Y}_i = (Y_{i1}, \ldots, Y_{iT})'$, and $\underline{u}_i = (u_{i1}, \ldots, u_{iT})'$,
we can reformulate this model as follows:

$$
  \underline{Y}_i = \underline{\mathbf{X}}_i \beta + \underline{u}_i, \quad \forall i.
$$

Now, we assume 

1. $E[\mathbf{X}'_{it}u_{it}] = 0$, $\forall i, t$. This assumption, called *(contempraneous) exogneity assumption*, implies that $u_{it}$ and $\mathbf{X}_{it}$ are orthogonal in the conditional mean sence, $E[u_{it} | \mathbf{X}_{it}] = 0$. However, this assumption does not imply $u_{it}$ is uncorrelated with the explanatory variables in all time periods (strictly exogeneity), that is, $E[u_{it} | \mathbf{X}_{i1}, \ldots, \mathbf{X}_{iT}] = 0$. This assumption palces no restriction on the relationship between $\mathbf{X}_{is}$ and $u_{it}$ for $s\not=t$.
1. $E[\underline{\mathbf{X}}'_i\underline{\mathbf{X}}_i] \succ 0$.

Under these two assumptions, the true parameter is given by
$$
  \beta = E[\underline{\mathbf{X}}'_i\underline{\mathbf{X}}_i]^{-1} E[\underline{\mathbf{X}}'_i\underline{Y}_i].
$$

Hence, the OLSE (pooled OLSE) is given by 
$$
  \hat{\beta} 
  = \left( \frac{1}{n} \sum_{i=1}^n \underline{\mathbf{X}}'_i\underline{\mathbf{X}}_i \right)^{-1}
  \left( \frac{1}{n} \sum_{i=1}^n \underline{\mathbf{X}}'_i\underline{Y}_i \right)
  = \left( \frac{1}{n} \sum_{i=1}^n \sum_{t=1}^T \mathbf{X}'_{it} \mathbf{X}_{it} \right)^{-1}
  \left( \frac{1}{n} \sum_{i=1}^n \sum_{t=1}^T \mathbf{X}'_{it} Y_{it} \right).
$$

Using the full matrix notation, the OLS estimator is 
$$
  \hat{\beta} = (\mathbf{X}' \mathbf{X})^{-1} (\mathbf{X}' Y),
$$
where $\mathbf{X} = (\underline{\mathbf{X}}_1, \ldots, \underline{\mathbf{X}}_n)'$ and 
$Y = (\underline{Y}_1, \ldots, \underline{Y}_n)'$.

In `R` programming, the `lm` function provides the pooled OLSE in the context of panel data model.
Another way is the `plm` function in the package `plm`.
When you want to estimate pooled OLS by the `plm` function, you need to specify `model = "pooling"`.
Moreover, you should specify individual and time index using `index` augment.
This augment passes `index = c("individual index", "time index")`.

```{r pooledOLSE}
bols1 <- lm(model, data = dt)

library(plm)
bols2 <- plm(model, data = dt, model = "pooling", index = c("id", "time"))
```

The pooled OLS estimator is consistent and asymptotically normally distributed.
$$
  \sqrt{n}(\hat{\beta} - \beta) \sim N(0, A^{-1} B A^{-1}),
$$
where $A = E[\underline{\mathbf{X}}'_i\underline{\mathbf{X}}_i]$ 
and $B = E[\underline{\mathbf{X}}'_i \underline{u}_i \underline{u}'_i \underline{\mathbf{X}}_i]$.
The consistent estimator of the asymptotic variance covariance matrix is given by 
$$
  \hat{A}^{-1} \hat{B} \hat{A}^{-1} = 
  \left( \frac{1}{n} \sum_{i=1}^n \underline{\mathbf{X}}'_i\underline{\mathbf{X}}_i \right)^{-1}
  \left( \frac{1}{n} \sum_{i=1}^n \underline{\mathbf{X}}'_i \underline{u}_i \underline{u}'_i \underline{\mathbf{X}}_i \right)
  \left( \frac{1}{n} \sum_{i=1}^n \underline{\mathbf{X}}'_i\underline{\mathbf{X}}_i \right)^{-1}
$$
Thus, estimator of asymptotic variance of the pooled OLSE is 
$$
  \hat{Avar}(\hat{\beta}) =
  \left( \sum_{i=1}^n \underline{\mathbf{X}}'_i\underline{\mathbf{X}}_i \right)^{-1}
  \left( \sum_{i=1}^n \underline{\mathbf{X}}'_i \underline{u}_i \underline{u}'_i \underline{\mathbf{X}}_i \right)
  \left( \sum_{i=1}^n \underline{\mathbf{X}}'_i\underline{\mathbf{X}}_i \right)^{-1}.
$$
Using the full matrix notations, we can reformulate
$$
  \hat{Avar}(\hat{\beta}) =
  (\mathbf{X}' \mathbf{X})^{-1}
  (\mathbf{X}' U \mathbf{X})
  (\mathbf{X}' \mathbf{X})^{-1},
$$
where
$$
  U = 
  \begin{pmatrix}
    \underline{u}_1 \underline{u}'_1 & \mathbf{0} & \cdots & \mathbf{0} \\
    \mathbf{0} & \underline{u}_2 \underline{u}'_2 & \cdots & \mathbf{0} \\
    \vdots & \vdots & \cdots & \vdots \\
    \mathbf{0} & \mathbf{0} & \cdots & \underline{u}_n \underline{u}'_n
  \end{pmatrix}.
$$
The standard errors calculated by this matrix is called *robust standard errors clustered by individuals*.

In `R`, the `lm` and `plm` function provide the standard errors based on $\hat{Avar}(\hat{\beta}) = \hat{\sigma}^2 (X'X)^{-1}$,
where $\hat{\sigma}^2 = \hat{u}\hat{u}'/(nT - K)$ and $\hat{u} = Y - X \hat{\beta}$.
There are two ways to obtain cluster robust standard errors.
The first way is to caclulate by yourself.
The second way is to use the `coeftest` function in the package `lmtest`.
When you use this function, 
we should use the `plm` function to estimate the pooled OLSE, and 
the `vcovHC` function (the package `sandwich`) in the `vcov` augment of `coeftest` fucntion.

```{r Inference of Pooled OLSE}
# Setup
N <- length(unique(dt$id)); T <- length(unique(dt$time))
X <- model.matrix(bols1); k <- ncol(X)

# Inference
uhat <- bols1$residuals
uhatset <- matrix(0, nrow = nrow(X), ncol = nrow(X))

i_from <- 1; j_from <- 1
for (i in 1:max(dt$id)) {
  x <- as.numeric(rownames(dt))[dt$id == i]
  usq <- uhat[x] %*% t(uhat[x])
  i_to <- i_from + nrow(usq) - 1
  j_to <- j_from + ncol(usq) - 1
  uhatset[i_from:i_to, j_from:j_to] <- usq
  i_from <- i_to + 1; j_from <- j_to + 1
}

Ahat <- t(X) %*% X
Bhat <- t(X) %*% uhatset %*% X
vcovols <- solve(Ahat) %*% Bhat %*% solve(Ahat)
seols <- sqrt(diag(vcovols))

# Easy way
library(lmtest)
library(sandwich)
easy_cluster <- coeftest(
  bols2, vcov = vcovHC(bols2, type = "HC0", cluster = "group"))
```

The result is shown in the first column of Table \ref{pdm}.
The partial effect of experience represents the percent change of wages.
Thus,
$$
  (\text{\% Change of Wage}) = 64.6 - 2 \cdot 1.3 \cdot \text{exp}.
$$
For example, wages increase by 12.99\% at a mathematical mean of labor experience (`exp`).
Moreover, this result implies diminishing marginal returns of labor experience.

## Feasible GLSE

Adding and assumption of the conditional variance of $\underline{u}_i$
allows for using the Generalized Ordinary Squares method.
To implement, we assume 

1. $E[\underline{X}_i \otimes \underline{u}_i] = 0$. A sufficient condition to satisfy this relationship is $E[ \underline{u}_i | \underline{X}_i] = 0$. This assumption implies $E[\underline{X}'_i \Omega^{-1} \underline{u}_i] = 0$ where $\Omega = E[\underline{u}_i\underline{u}'_i]$ is $T \times T$ matrix.
1. $\Omega \succ 0$ and E[\underline{X}'_i \Omega^{-1} \underline{X}'_i] \succ 0$.

The GLS estimator is given by 
$$
  \hat{\beta}_{GLS} 
  = \left( \frac{1}{n} \sum_{i=1}^n \underline{\mathbf{X}}'_i \Omega^{-1} \underline{\mathbf{X}}_i \right)^{-1}
  \left( \frac{1}{n} \sum_{i=1}^n \underline{\mathbf{X}}'_i \Omega^{-1} \underline{Y}_i \right).
$$
Under two assumptions, this estimator is weakly consistent.

In the feasible GLS method,
we replace the unknown $\Omega$ with a consistent estimator.
Here, we consider the two-step FGLS:
obtain the OLS estimator and residuals; replace $\Omega$ by it.
Then, the unknown $\Omega$ is replaced by 
$$
  \hat{\Omega} = \frac{1}{n} \sum_{i=1}^n \underline{\hat{u}}_i\underline{\hat{u}}'_i,
$$
where $\underline{\hat{u}}_i = \underline{Y}_i - \underline{\mathbf{X}}_i \hat{\beta}_{OLS}$.

Thus, the FGLS estimator is 
$$
  \hat{\beta}_{FGLS} 
  = \left( \frac{1}{n} \sum_{i=1}^n \underline{\mathbf{X}}'_i \hat{\Omega}^{-1} \underline{\mathbf{X}}_i \right)^{-1}
  \left( \frac{1}{n} \sum_{i=1}^n \underline{\mathbf{X}}'_i \hat{\Omega}^{-1} \underline{Y}_i \right).
$$
Using the full matrix notations,
$$
  \hat{\beta}_{FGLS}
  = \{ \mathbf{X}'(I_n \otimes \hat{\Omega}^{-1}) \mathbf{X} \}^{-1}
  \{ \mathbf{X}'(I_n \otimes \hat{\Omega}^{-1}) Y \}.
$$

In the `R` programming, 
there are two ways to obtain the FGLS estimator.
The first way is to caclulate by yourself.
The second way is to use the `pggls` function in the package `plm`.
When you use the `pggls` function,
you should specify individual and time indexs using `index` augment,
and type in `model = "pooling"`.

```{r FGLS estimator}
# Setup
X <- model.matrix(model, dt); k <- ncol(X)
y <- dt$lwage
N <- length(unique(dt$id)); T <- length(unique(dt$time))

# Estimator of Omega
uhat <- bols1$residuals

Omega_sum <- matrix(0, ncol = T, nrow = T)
for (i in 1:N) {
  x <- as.numeric(rownames(dt))[dt$id == i]
  Omega_sum <- uhat[x] %*% t(uhat[x]) + Omega_sum
}
Omega <- Omega_sum/N

# FGLS estimator
kroOmega <- diag(N) %x% solve(Omega)
bfgls <- solve(t(X) %*% kroOmega %*% X) %*% (t(X) %*% kroOmega %*% y)

# Easy way!!!
easy_fgls <- pggls(
  model, data = dt, index = c("id", "time"), model = "pooling")
```

The asymptotic distribution of the FGLS estimator is given by 
$$
  \sqrt{n}(\hat{\beta}_{FGLS} - \beta) \sim N(0, A^{-1} B A^{-1}),
$$
where $A = E[\underline{\mathbf{X}}'_i \Omega^{-1} \underline{\mathbf{X}}_i]$ 
and $B = E[\underline{\mathbf{X}}'_i \Omega^{-1} \underline{u}_i \underline{u}'_i \Omega^{-1} \underline{\mathbf{X}}_i]$.
The consistent estimator of $A$ and $B$ is 
$$
  \hat{A} = \frac{1}{n} \sum_{i=1}^n \underline{\mathbf{X}}'_i \hat{\Omega}^{-1} \underline{\mathbf{X}}_i,
$$
$$
  \hat{B} 
  = \frac{1}{n} \sum_{i=1}^n
  \underline{\mathbf{X}}'_i \hat{\Omega}^{-1} 
  \underline{u}^{FLGS}_i \underline{u}^{FGLS'}_i 
  \hat{\Omega}^{-1} \underline{\mathbf{X}}_i,
$$
where $\underline{u}^{FLGS}_i = \underline{Y}_i - \underline{\mathbf{X}}_i \hat{\beta}_{FGLS}$.
Thus, estimator of asymptotic variance of the pooled OLSE is 
$$
  \hat{Avar}(\hat{\beta}_{FGLS}) =
  \left( \sum_{i=1}^n \underline{\mathbf{X}}'_i \hat{\Omega}^{-1} \underline{\mathbf{X}}_i \right)^{-1}
  \left( \sum_{i=1}^n \underline{\mathbf{X}}'_i \hat{\Omega}^{-1} 
  \underline{u}^{FLGS}_i \underline{u}^{FGLS'}_i 
  \hat{\Omega}^{-1} \underline{\mathbf{X}}_i \right)
  \left( \sum_{i=1}^n \underline{\mathbf{X}}'_i \hat{\Omega}^{-1} \underline{\mathbf{X}}_i \right)^{-1}.
$$
Using the full matrix notations, 
$$
  \hat{Avar}(\hat{\beta}_{FGLS}) =
  \{ \mathbf{X}'(I_n \otimes \hat{\Omega}^{-1}) \mathbf{X} \}^{-1}
  \{ \mathbf{X}'(I_n \otimes \hat{\Omega}^{-1}) U (I_n \otimes \hat{\Omega}^{-1}) \mathbf{X} \}^{-1}
  \{ \mathbf{X}'(I_n \otimes \hat{\Omega}^{-1}) \mathbf{X} \}^{-1},
$$
where
$$
  U = 
  \begin{pmatrix}
    \underline{u}^{FLGS}_1 \underline{u}^{FGLS'}_1 & \mathbf{0} & \cdots & \mathbf{0} \\
    \mathbf{0} & \underline{u}^{FLGS}_2 \underline{u}^{FGLS'}_2 & \cdots & \mathbf{0} \\
    \vdots & \vdots & \cdots & \vdots \\
    \mathbf{0} & \mathbf{0} & \cdots & \underline{u}^{FLGS}_n \underline{u}^{FGLS'}_n
  \end{pmatrix}.
$$

In the `R` programming,
you need to caclculate by yourself.
The `pggls` function provides the FGLS estimator.
However, this function calculates standard errors, assuming *system homoskedasticity*, that is,
$E[\underline{\mathbf{X}}'_i \Omega^{-1} \underline{u}_i \underline{u}'_i \Omega^{-1} \underline{\mathbf{X}}_i] = E[\underline{\mathbf{X}}'_i \Omega^{-1} \underline{\mathbf{X}}_i]$.
If you can rationale this assumption, 
the `bggls` fucntion is the easiest way to carry out statistical inference.

```{r Inference of FGLS estimator}
ufgls <- y - X %*% bfgls
uhatset <- matrix(0, nrow = nrow(X), ncol = nrow(X))
i_from <- 1; j_from <- 1
for (i in 1:max(dt$id)) {
  x <- as.numeric(rownames(dt))[dt$id == i]
  usq <- uhat[x] %*% t(uhat[x])
  i_to <- i_from + nrow(usq) - 1
  j_to <- j_from + ncol(usq) - 1
  uhatset[i_from:i_to, j_from:j_to] <- usq
  i_from <- i_to + 1; j_from <- j_to + 1
}

Ahat <- t(X) %*% kroOmega %*% X
Bhat <- t(X) %*% kroOmega %*% uhatset %*% kroOmega %*% X
vcovfgls <- solve(Ahat) %*% Bhat %*% solve(Ahat)
sefgls <- sqrt(diag(vcovfgls))
```

The result is shown in the second column of Table \ref{pdm}.
The partial effect of experience represents the percent change of wages.
Thus,
$$
  (\text{\% Change of Wage}) = 52.9 - 2 \cdot 0.9 \cdot \text{exp}.
$$
For example, wages increase by 17.17\% at a mathematical mean of labor experience (`exp`).

## Fixed Effect Model

To examine the effect of labor experience on wages,
we introduce unobserved heterogeneity such as ability.
The unobserved effects model is given by 
$$
  \text{lwage}_{it} = 
  \beta_1 \cdot \text{exp}_{it} +
  \beta_2 \cdot \text{sqexp}_{it} + 
  c_i + u_{it},
$$
where $c_i$ is unobserved component which is constant over time, 
$u_{it}$ is the idiosyncratic error term.
The fixed effect model treats $c_i$ as a parameter to be estimated for each cross section unit $i$.

We generalize the unobserved effects model as follows:
$$
  Y_{it} = \mathbf{X}_{it} \beta + c_i + u_{it}, \quad t = 1, \ldots, T, \quad i = 1, \ldots, n.
$$
Using notations $\underline{\mathbf{X}}_i = (\mathbf{X}'_{i1}, \ldots, \mathbf{X}'_{iT})'$
and $\underline{Y}_i = (Y_{i1}, \ldots, Y_{iT})'$, and $\underline{u}_i = (u_{i1}, \ldots, u_{iT})'$,
we can reformulate this model as follows:
$$
  \underline{Y}_i = \underline{\mathbf{X}}_i \beta + \iota c_i + \underline{u}_i, \quad \forall i,
$$
where $\iota = (1, \ldots, 1)'$ is $T \times 1$ vector.

To implement the fixed effect model,
we assume the following three assumptions:

1. Strict exogeneity: $E[u_{it} | \mathbf{X}_{i1}, \ldots, \mathbf{X}_{iT}, c_i] = 0$.
1. Full rank: $rank(\sum_t E[\ddot{\mathbf{X}}'_{it}\ddot{\mathbf{X}}_{it}]) = rank(E[\ddot{\underline{\mathbf{X}}}'_i\ddot{\underline{\mathbf{X}}}_i]) = K$ where $\ddot{\mathbf{X}}_{it} = \mathbf{X}_{it} - T^{-1}\sum_t \mathbf{X}_{it}$ is *time-demeaning matrix*.
1. homoskedasticity: $E[\underline{u}_i \underline{u}'_i|\mathbf{X}_{i1}, \ldots, \mathbf{X}_{iT}, c_i] = \sigma^2_u I_T$.

To obtain the FE estimator, we consider the within transformation first.
Averaging the unobserved effects model for individual $i$ and time $t$ over time yields
$$
  \bar{Y}_i = \bar{\mathbf{X}}_i \beta + c_i + \bar{u}_i,
$$
where $\bar{Y}_i = T^{-1} \sum_t Y_{it}$, $\bar{\mathbf{X}}_i = T^{-1} \sum_t \mathbf{X}_{it}$,
and $\bar{u}_i = T^{-1} \sum_t u_{it}$.
Subtracting this equation from the original one for each t yields
$$
  Y_{it} - \bar{Y}_i = (\mathbf{X}_{it} - \bar{\mathbf{X}}_i) \beta + (u_{it} - \bar{u}_i)
  \Leftrightarrow
  \ddot{Y}_{it} = \ddot{\mathbf{X}}_{it} \beta + \ddot{u}_{it}.
$$
Note that $E[\ddot{u}_{it} | \ddot{\mathbf{X}}_{i1}, \ldots, \ddot{\mathbf{X}}_{iT}] = 0$ 
under the first assumption.
Using the $T$ system of equation, the within transformation is 
$$
  Q_T \underline{Y}_i = Q_T \underline{\mathbf{X}}_i \beta + Q_T \underline{u}_i
  \Leftrightarrow
  \ddot{\underline{Y}}_i = \ddot{\underline{\mathbf{X}}}_i \beta + \ddot{\underline{u}}_i.
$$
where $Q_T = I_T - \iota (\iota' \iota)^{-1} \iota$ and $Q_T \iota = 0$.
Using the matrix notations, the within transformation is 
$$
  (I_n \otimes Q_t) Y = (I_n \otimes Q_t) X \beta + (I_n \otimes Q_t) u
  \Leftrightarrow
  \ddot{Y} = \ddot{X} \beta + \ddot{u}.
$$

The FE estimator is given by 
$$
  \hat{\beta}_{FE} = 
  \left( \frac{1}{n} \sum_{i=1}^n \ddot{\underline{\mathbf{X}}}'_i 
  \ddot{\underline{\mathbf{X}}}_i \right)^{-1}
  \left( \frac{1}{n} \sum_{i=1}^n \ddot{\underline{\mathbf{X}}}'_i \ddot{\underline{Y}}_i \right)
  = (\ddot{X}' \ddot{X})^{-1}(\ddot{X}' \ddot{Y}).
$$

In the `R` programming,
there are two ways to obtain the FE estimator.
The first way is to caclulate by yourself.
The second way is to use the `plm` function.
When you use the `plm` function,
you need to specify `model = "within"` to implement the FE model.

```{r FE estimator}
# Setup
X <- model.matrix(model, dt); k <- ncol(X)
y <- dt$lwage
N <- length(unique(dt$id)); T <- length(unique(dt$time))

# FE estimator
i <- rep(1, T)
Qt <- diag(T) - i %*% solve(t(i) %*% i) %*% t(i)
Ydev <- diag(N) %x% Qt %*% y
Xdev <- diag(N) %x% Qt %*% X
bfe <- solve(t(Xdev) %*% Xdev) %*% t(Xdev) %*% Ydev

# Awesome way !!!
plmfe <- plm(model, data = dt, index = c("id", "time"), model = "within")
```

Under the third assumption, asymptotic distribution of the FE estimator is given by 
$$
  \sqrt{n}(\hat{\beta}_{FE} - \beta) 
  \sim N(0, \sigma^2_u E[\ddot{\underline{\mathbf{X}}}'_i \ddot{\underline{\mathbf{X}}}_i]).
$$
The consistent estimator of the asymptotic variance of the FE estimator is 
$$
  \hat{Avar}(\hat{\beta}_{FE}) =
  \hat{\sigma}_u^2 
  \left( \sum_{i=1}^n \ddot{\underline{\mathbf{X}}}'_i \ddot{\underline{\mathbf{X}}}_i \right)^{-1} =
  \hat{\sigma}_u^2 (\ddot{X}' \ddot{X})^{-1},
$$
where $\hat{\sigma}_u^2 = \frac{1}{n(T-1)-K} \sum_i \sum_t \hat{\ddot{u}}_{it}$,
and $\hat{\ddot{u}}_{it} = \ddot{Y}_{it} - \ddot{\mathbf{X}}_{it} \hat{\beta}_{FE}$.

In the `R` programming,
the `plm` function also returns standard errors, $\hat{\sigma}_u^2 (\ddot{X}' \ddot{X})^{-1}$.
Of course, you can compute the standard errors manually.
The sample code is as follows:

```{r Inference of FE estimator}
uhat <- Ydev - Xdev %*% bfe
sigmahat <- sum(uhat^2)/(N*(T-1)-k)
vcovfe <- sigmahat * solve(t(Xdev) %*% Xdev)
sefe <- sqrt(diag(vcovfe))
```

The result is shown in the third column in Table \ref{pdm}.
The partial effect of experience represents the percent change of wages.
Thus,
$$
  (\text{\% Change of Wage}) = 11.4 - 2 \cdot 0.04 \cdot \text{exp}.
$$
For example, wages increase by 9.812\% at a mathematical mean of labor experience (`exp`).


```{r table, results = "asis", echo = FALSE}
library(stargazer)
stargazer(
  bols1, bols1, bols1, #bols1, 
  column.labels = c("Pooled OLS", "FGLS", "Fixed Effect"), #"Random Effect"),
  coef = list(coef(bols1), bfgls, bfe), #bre),
  se = list(seols, sefgls, sefe), #sere),
  keep.stat = c("n"), report = "vcs",
  omit.table.layout = "n", table.placement = "t",
  header = FALSE,
  type = "latex",
  title = "Panel Data Model: Effect of Experience on Wages",
  label = "pdm"
)
```